services:
  # ... секции mongo, clickhouse, backend, frontend остаются БЕЗ ИЗМЕНЕНИЙ ...
  mongo:
    image: mongo:latest
    container_name: scb_mongo
    ports: ["27017:27017"]
    volumes: [mongo_data:/data/db]
    env_file: [./.env]
    restart: always
    healthcheck: { test: ["CMD", "mongosh", "--eval", "db.adminCommand('ping')"], interval: 10s, timeout: 5s, retries: 5 }
    networks: [scb_network]

  clickhouse:
    image: clickhouse/clickhouse-server:latest
    container_name: scb_clickhouse
    ports: ["8123:8123", "9000:9000"]
    volumes: [clickhouse_data:/var/lib/clickhouse]
    env_file: [./.env]
    restart: always
    ulimits: { nofile: { soft: 262144, hard: 262144 } }
    healthcheck: { test: ["CMD", "wget", "--spider", "-q", "http://localhost:8123/ping"], interval: 10s, timeout: 5s, retries: 5 }
    networks: [scb_network]

  backend:
    build: { context: ../.., dockerfile: ./infra/docker/backend/Dockerfile }
    container_name: scb_backend
    ports: ["8000:8000"]
    volumes: ["../../backend:/app"]
    env_file: [./.env]
    command: poetry run uvicorn main:app --host 0.0.0.0 --port 8000 --reload
    depends_on: { mongo: { condition: service_healthy }, clickhouse: { condition: service_healthy }, airflow-webserver: { condition: service_healthy } }
    restart: always
    networks: [scb_network]

  frontend:
    build: { context: ../.., dockerfile: ./infra/docker/frontend/Dockerfile }
    container_name: scb_frontend
    ports: ["5173:5173"]
    volumes: ["../../frontend:/app", /app/node_modules]
    env_file: [./.env]
    command: npm run dev -- --host
    restart: always
    depends_on: [backend]
    networks: [scb_network]

  # --- Airflow Services ---
  postgres:
    image: postgres:13
    container_name: scb_airflow_postgres
    restart: always
    env_file: [./.env]
    volumes: [airflow_postgres_data:/var/lib/postgresql/data]
    healthcheck: { test: ["CMD-SHELL", "pg_isready -U $${POSTGRES_USER} -d $${POSTGRES_DB}"], interval: 10s, timeout: 5s, retries: 5 }
    networks: [scb_network]

  redis:
    image: redis:latest
    container_name: scb_airflow_redis
    restart: always
    healthcheck: { test: ["CMD", "redis-cli", "ping"], interval: 10s, timeout: 5s, retries: 5 }
    networks: [scb_network]
  
  airflow-init:
    build: { context: ../.., dockerfile: ./infra/docker/etl/Dockerfile }
    container_name: scb_airflow_init
    profiles: ["setup"]
    environment:
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=${POSTGRES_DB}
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
    depends_on: { postgres: { condition: service_healthy }, redis: { condition: service_healthy } }
    command: -c "airflow db migrate && (airflow users create --username airflow --password airflow --firstname Airflow --lastname Admin --role Admin --email airflow@example.com || true)"
    networks: [scb_network]
    
  airflow-webserver:
    build: { context: ../.., dockerfile: ./infra/docker/etl/Dockerfile }
    container_name: scb_airflow_webserver
    restart: always
    environment:
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=${POSTGRES_DB}
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__CELERY__BROKER_URL=${AIRFLOW__CELERY__BROKER_URL}
      - AIRFLOW__CELERY__RESULT_BACKEND=${AIRFLOW__CELERY__RESULT_BACKEND}
      - AIRFLOW_UID=${AIRFLOW_UID}
      - AIRFLOW_GID=${AIRFLOW_GID}
    ports: ["8080:8080"]
    volumes: ["../../etl/dags:/opt/airflow/dags", airflow_logs:/opt/airflow/logs]
    depends_on: { postgres: { condition: service_healthy }, redis: { condition: service_healthy } }
    command: webserver
    healthcheck: { test: ["CMD-SHELL", "curl --fail http://localhost:8080/health"], interval: 30s, timeout: 30s, retries: 5, start_period: 60s }
    networks: [scb_network]

  airflow-scheduler:
    build: { context: ../.., dockerfile: ./infra/docker/etl/Dockerfile }
    container_name: scb_airflow_scheduler
    restart: always
    environment:
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=${POSTGRES_DB}
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__CELERY__BROKER_URL=${AIRFLOW__CELERY__BROKER_URL}
      - AIRFLOW__CELERY__RESULT_BACKEND=${AIRFLOW__CELERY__RESULT_BACKEND}
      - AIRFLOW_UID=${AIRFLOW_UID}
      - AIRFLOW_GID=${AIRFLOW_GID}
    volumes: ["../../etl/dags:/opt/airflow/dags", airflow_logs:/opt/airflow/logs]
    depends_on: { airflow-webserver: { condition: service_healthy } }
    command: scheduler
    networks: [scb_network]

  airflow-worker:
    build: { context: ../.., dockerfile: ./infra/docker/etl/Dockerfile }
    container_name: scb_airflow_worker
    restart: always
    environment:
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=${POSTGRES_DB}
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__CELERY__BROKER_URL=${AIRFLOW__CELERY__BROKER_URL}
      - AIRFLOW__CELERY__RESULT_BACKEND=${AIRFLOW__CELERY__RESULT_BACKEND}
      - AIRFLOW_UID=${AIRFLOW_UID}
      - AIRFLOW_GID=${AIRFLOW_GID}
    volumes: ["../../etl/dags:/opt/airflow/dags", airflow_logs:/opt/airflow/logs]
    depends_on: { airflow-scheduler: { condition: service_started } }
    command: celery worker
    networks: [scb_network]

volumes:
  mongo_data:
  clickhouse_data:
  airflow_postgres_data:
  airflow_logs:

networks:
  scb_network:
    driver: bridge